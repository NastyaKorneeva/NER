{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af7dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import bleach\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72887bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vacancy_from_hh(base_url = 'https://api.hh.ru/vacancies?text=', \n",
    "                    search_query = 'аналитик',\n",
    "                    pages_number = 20,\n",
    "                    per_page = 100):\n",
    "    data = []\n",
    "    salary_data = pd.DataFrame()\n",
    "    for page_number in tqdm(range(pages_number)):\n",
    "        request = requests.get(base_url + search_query, {'per_page': str(per_page), \n",
    "                                                         'page': page_number, \n",
    "                                                         'only_with_salary':'true'})\n",
    "        json_data = request.json()\n",
    "        salary_data = pd.concat([salary_data, pd.json_normalize(json_data, 'items')[\n",
    "            ['id', 'salary.from', 'salary.to', 'salary.currency']\n",
    "        ]])\n",
    "        if 'items' not in json_data:\n",
    "            continue\n",
    "        for short_vacancy_data in json_data['items']:\n",
    "\n",
    "            vacancy_data = requests.get(short_vacancy_data.get('url')).json()\n",
    "            data.append(vacancy_data)\n",
    "        \n",
    "    return pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a92173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst = vacancy_from_hh(search_query = 'Аналитик')\n",
    "electrician = vacancy_from_hh(search_query = 'Электрик')\n",
    "accountant = vacancy_from_hh(search_query = 'Бухгалтер')\n",
    "waiter = vacancy_from_hh(search_query = 'Официант')\n",
    "administrator = vacancy_from_hh(search_query = 'Администратор')\n",
    "security = vacancy_from_hh(search_query = 'Охранник')\n",
    "developer = vacancy_from_hh(search_query = 'Разработчик')\n",
    "\n",
    "analyst['query_profession'] = 'Аналитик'\n",
    "electrician['query_profession'] = 'Электрик'\n",
    "accountant['query_profession'] = 'Бухгалтер'\n",
    "waiter['query_profession'] = 'Официант'\n",
    "administrator['query_profession'] = 'Администратор'\n",
    "security['query_profession'] = 'Охранник'\n",
    "developer['query_profession'] = 'Разработчик'\n",
    "\n",
    "data = pd.concat([analyst, electrician, accountant, waiter, administrator, security, developer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c6a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancy_hh = pd.read_json('all_professions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d8a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bleach.clean(vacancy_hh.description[0], tags=[], strip=True)\n",
    "\n",
    "from googletrans import Translator\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "skill_hard = pd.read_csv('hardskills.txt')\n",
    "\n",
    "translator = GoogleTranslator(source='en', target='ru')\n",
    "skill_hard['ru_skills'] = skill_hard['Skill'].progress_apply(translator.translate)\n",
    "\n",
    "skill_hard['ru_skill_a'] = skill_hard['ru_skills'].apply(preprocess_text)\n",
    "\n",
    "skill_hard.to_excel(\"skill_hard.xlsx\", index = False)\n",
    "skill_hard = pd.read_excel(\"skill_hard.xlsx\")\n",
    "\n",
    "def contains_skills(row):\n",
    "    return pd.DataFrame([1 if re.search(f\"{row}\", vacancy_hh.description[i]) else 0 \\\n",
    "              for i in range(len(vacancy_hh.description))], \n",
    "            columns=[row])\n",
    "\n",
    "skill_soft = pd.read_csv('skills_index_final.csv')\n",
    "\n",
    "translator = GoogleTranslator(source='en', target='ru')\n",
    "skill_soft['Skill'] = skill_soft['Skill'].str.replace('ability', '')\n",
    "skill_soft['ru_skills'] = skill_soft['Skill'].progress_apply(translator.translate)\n",
    "\n",
    "skill_soft['ru_skill_a'] = skill_soft['ru_skills'].apply(preprocess_text)\n",
    "\n",
    "skill_soft.to_excel(\"skill_soft.xlsx\", index = False)\n",
    "skill_soft = pd.read_excel(\"skill_soft.xlsx\")\n",
    "\n",
    "skill_soft_one_hot = pd.concat(skill_soft['ru_skill_a'].map(contains_skills).tolist(), axis=1)\n",
    "skill_soft_one_hot_ru = skill_soft_one_hot[skill_soft_one_hot.columns[skill_soft_one_hot.sum() != 0]]\n",
    "one_hot_skill = skill_soft_one_hot_ru.loc[:,~skill_soft_one_hot_ru.columns.duplicated()]\n",
    "\n",
    "skill_hard_one_hot_ru = pd.concat(skill_hard['ru_skill_a'].map(contains_skills).tolist(), axis=1)\n",
    "skill_hard_one_hot_ru = skill_hard_one_hot_ru[skill_hard_one_hot_ru.columns[skill_hard_one_hot_ru.sum() != 0]]\n",
    "skill_hard_one_hot = pd.concat(skill_hard['Skill'].map(contains_skills).tolist(), axis=1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "import datasets\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "\n",
    "data_500 = pd.read_excel('/content/drive/MyDrive/lbgkjv/tt (1).xlsx')\n",
    "\n",
    "data_500 = data_500.rename(\n",
    "  columns={\n",
    "      '-DOCSTART-': 'tokens',\n",
    "      'O': 'ner_tags'\n",
    "  }\n",
    ")\n",
    "\n",
    "data_train_tokens = np.split(data_500['tokens'].to_numpy(),\n",
    "                             np.where(data_500['tokens'].to_numpy() == '\"')[0])[1::2]\n",
    "\n",
    "data_train_ner_tags = np.split(data_500['ner_tags'].to_numpy(),\n",
    "                               np.where(data_500['tokens'].to_numpy() == '\"')[0])[1::2]\n",
    "\n",
    "data_for_ner = pd.DataFrame([data_train_tokens, data_train_ner_tags]).transpose().reset_index()\n",
    "\n",
    "data_for_ner = data_for_ner.rename(\n",
    "    columns={\n",
    "        'index': 'id',\n",
    "        0: 'tokens',\n",
    "        1: 'ner_tags'\n",
    "    }\n",
    ")\n",
    "\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-Hard\",\n",
    "    2: \"I-Hard\",\n",
    "    3: \"B-Soft\",\n",
    "    4: \"I-Soft\",\n",
    "    5: \"B-Another\",\n",
    "    6: \"I-Another\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-Hard\": 1,\n",
    "    \"I-Hard\": 2,\n",
    "    \"B-Soft\": 3,\n",
    "    \"I-Soft\": 4,\n",
    "    \"B-Another\": 5,\n",
    "    \"I-Another\": 6,\n",
    "}\n",
    "\n",
    "data_train_ner_tags_new = [np.array([label2id.get(i, i) for i in data_train_ner_tags[j]])\n",
    "                           for j in range(len(data_train_ner_tags))]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_train_tokens, data_train_ner_tags_new,\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=42)\n",
    "data_train = pd.DataFrame([X_train, y_train]).transpose().reset_index().rename(\n",
    "    columns={\n",
    "        'index': 'id',\n",
    "        0: 'tokens',\n",
    "        1: 'ner_tags'\n",
    "    }\n",
    ")\n",
    "\n",
    "data_test = pd.DataFrame([X_test, y_test]).transpose().reset_index().rename(\n",
    "    columns={\n",
    "        'index': 'id',\n",
    "        0: 'tokens',\n",
    "        1: 'ner_tags'\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset_train = Dataset(pa.Table.from_pandas(data_train))\n",
    "dataset_test = Dataset(pa.Table.from_pandas(data_test))\n",
    "data_final = datasets.DatasetDict({\"train\":dataset_train,\"test\":dataset_test})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "example = data_final['train'][0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7182aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_wnut = data_final.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf3d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd6334",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [*label2id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3772e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\", num_labels=7, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50280a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_wnut_model_\",\n",
    "    learning_rate=8.886156780071567e-05,\n",
    "    per_device_train_batch_size=15,\n",
    "    per_device_eval_batch_size=7,\n",
    "    num_train_epochs=40,\n",
    "    weight_decay=.0060777108654959874,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=0.1,\n",
    "    #metric_for_best_model = 'f1',\n",
    "    #load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_wnut[\"train\"],\n",
    "    eval_dataset=tokenized_wnut[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed44fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5161f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_wnut[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde3a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn_crfsuite\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107bcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/davidsbatista/NER-Evaluation.git\n",
    "!mv NER-Evaluation/ner_evaluation ner_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_evaluation.ner_eval import collect_named_entities\n",
    "from ner_evaluation.ner_eval import compute_metrics\n",
    "from ner_evaluation.ner_eval import compute_precision_recall_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_named_entities(true_labels)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c10b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee59a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec68633",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-Hard\",\n",
    "    2: \"I-Hard\",\n",
    "    3: \"B-Soft\",\n",
    "    4: \"I-Soft\",\n",
    "    5: \"B-Another\",\n",
    "    6: \"I-Another\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-Hard\": 1,\n",
    "    \"I-Hard\": 2,\n",
    "    \"B-Soft\": 3,\n",
    "    \"I-Soft\": 4,\n",
    "    \"B-Another\": 5,\n",
    "    \"I-Another\": 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c790da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"cointegrated/rubert-tiny2\",\n",
    "                                                        num_labels=7, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d954dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/content/drive/MyDrive/lbgkjv/model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06257f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "\"Привет ! Ищем в команду Alanbase талантливого Frontend-разработчика на React уровня\n",
    "не ниже Middle+ с широким кругозором и искренней любовью к своему делу . Мы ждем ,\n",
    "что наш новый коллега поможет разрабатывать и поддерживать наш продукт , используя\n",
    "самые современные технологии и методы разработки и самое главное - ему это будет нравится .\n",
    "Что нужно делать : Разработка и поддержка пользовательского интерфейса сервиса Оптимизация\n",
    "интерфейса для обеспечения высокой производительности и отзывчивости Участие в принятии\n",
    "архитектурных и технических решений по разработке программного обеспечения Взаимодействие с\n",
    "разработчиками , системным аналитиками Помощь в улучшении процесса разработки Нам важно :\n",
    "Опыт от 3 лет Знание и опыт работы с TypeScript , React и Redux , с RESTful API , с Docker\n",
    "Понимание адаптивного и кросс-браузерного дизайна Опыт оценки и принятия архитектурных решений\n",
    "Опыт оптимизации производительности веб-приложений Желательно , но не обязательно Опыт работы с\n",
    "фреймворком Next.js Знание принципов UX/UI дизайна Понимание основ бэкенд-разработки\n",
    "Знакомство с affiliate сферой Что предлагаем : Оформление : ТК , ГПХ , ИП У нас\n",
    "перспективы для личностного и профессионального развития , динамичная и креативная\n",
    "рабочая среда , вдохновляющие проекты и возможности , и , важно , удаленка с гибким\n",
    "началом рабочего дня : )\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6899a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy='average', \n",
    "                      device=0, batch_size=2)\n",
    "pp = classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f873706",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_label = pd.read_csv('/content/drive/MyDrive/lbgkjv/description_without_index.txt', \n",
    "                             header=None).iloc[:500][0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d6cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe = pd.DataFrame(classifier(data_for_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cddd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "classi_data = classifier(data_for_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eddae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = pd.read_csv('/content/drive/MyDrive/lbgkjv/description_with_index.txt', \n",
    "                        header=None).iloc[:500][0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96503d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for i in range(len(dict_data)):\n",
    "    dataset.append(list(map(lambda x: {**x, **{'id': dict_data[i]}}, classi_data[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec717b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacab442",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssss1 = pd.Series()\n",
    "for i in range(pd.DataFrame(dataset).columns.shape[0]):\n",
    "    ssss1 = pd.concat([pd.DataFrame(dataset).iloc[i][pd.DataFrame(dataset).iloc[i].notna()],\n",
    "                    ssss1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5fd81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final = pd.DataFrame(ssss1.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a9ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hh = pd.read_json('/content/drive/MyDrive/lbgkjv/all_professions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ca4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final = dataset_final.merge(data_hh.reset_index()[\n",
    "    ['index', 'name', 'key_skills', 'salary.from',\n",
    "     'salary.to', 'salary.currency', 'salary.gross', 'description', 'query_profession']\n",
    "    ], left_on='id', right_on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final['salary.currency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final.loc[~dataset_final['salary.gross'], 'salary.from'] = dataset_final.loc[~dataset_final['salary.gross'], 'salary.from'] * 100/87\n",
    "dataset_final.loc[~dataset_final['salary.gross'], 'salary.to'] = dataset_final.loc[~dataset_final['salary.gross'], 'salary.to'] * 100/87\n",
    "dataset_final.loc[~dataset_final['salary.gross'], 'salary.gross'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8cdd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final.loc[dataset_final['salary.from'].isna(), 'salary'] = dataset_final.loc[dataset_final['salary.from'].isna(), 'salary.to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4497920",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final.loc[dataset_final['salary.to'].isna(), 'salary'] = dataset_final.loc[dataset_final['salary.to'].isna(), 'salary.from']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e836907",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final.loc[dataset_final['salary'].isna(), 'salary'] = (dataset_final.loc[dataset_final['salary'].isna(), 'salary.to'] +\n",
    "    dataset_final.loc[dataset_final['salary'].isna(), 'salary.from']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b37bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final.to_excel('/content/drive/MyDrive/lbgkjv/dataset_final.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30371261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_plt = dataset_final[['id', 'query_profession', 'salary.from', \n",
    "                         'salary.to']].drop_duplicates('id')[['query_profession', 'salary.from', 'salary.to']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b1123",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_plt.boxplot(by='query_profession', rot = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29606866",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.mean(\n",
    "    dataset_final.loc[dataset_final['salary'].isna(), 'salary.to'],\n",
    "    dataset_final.loc[dataset_final['salary'].isna(), 'salary.from'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe052b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssss = pd.Series()\n",
    "for i in range(data_pipe.columns.shape[0]):\n",
    "    ssss = pd.concat([data_pipe.iloc[i][data_pipe.iloc[i].notna()],\n",
    "                    ssss], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_an = pd.DataFrame(ssss.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14abd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hard = data_an.query('entity_group == \"Hard\"')\n",
    "data_soft = data_an.query('entity_group == \"Soft\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931192a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "strr = ' '.join(data_hard['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7feea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud = WordCloud(width = 10000, height = 10000, random_state=1, background_color='black',\n",
    "                      colormap='Set2', collocations=False).generate(strr)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(wordCloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8581a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = open('/content/drive/MyDrive/lbgkjv/stop-ru.txt', 'r', encoding='utf8')\n",
    "stop_words = stop_words.read()\n",
    "stop_words = stop_words.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d96a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_data=[]\n",
    "for i in strr.split():\n",
    "    if(i not in stop_words):\n",
    "        clear_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff00c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_data_str = strr = ' '.join(clear_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2436f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud = WordCloud(width = 10000, height = 10000, random_state=1,\n",
    "                      background_color='black', colormap='Set2', collocations=False).generate(clear_data_str)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(wordCloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de17a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "strr_s = ' '.join(data_soft['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd168e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud = WordCloud(width = 10000, height = 10000, random_state=1, background_color='black',\n",
    "                      colormap='Set2', collocations=False).generate(strr_s)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(wordCloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_data_s=[]\n",
    "for i in strr_s.split():\n",
    "    if(i not in stop_words):\n",
    "        clear_data_s.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adce0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_data_str_s = strr = ' '.join(clear_data_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fba18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud = WordCloud(width = 10000, height = 10000, random_state=1,\n",
    "                      background_color='black', colormap='Set2', collocations=False).generate(clear_data_str_s)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(wordCloud)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
